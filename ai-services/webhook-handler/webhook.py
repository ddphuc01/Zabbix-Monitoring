#!/usr/bin/env python3
"""
Zabbix AI Webhook Handler - Groq & Ansible Integration
Receives alerts from Zabbix, gathers system metrics via Ansible, and analyzes them using Groq API
"""

import os
import sys
import json
import hashlib
import time
import subprocess
import logging
from datetime import datetime
from flask import Flask, request, jsonify
from groq import Groq
import redis
import requests
from functools import wraps

# Configuration
GROQ_API_KEY = os.getenv('GROQ_API_KEY', '')
REDIS_HOST = os.getenv('REDIS_HOST', 'redis')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
CACHE_TTL = int(os.getenv('CACHE_TTL', 3600))
MAX_TOKENS = int(os.getenv('MAX_TOKENS', 200))
TEMPERATURE = float(os.getenv('TEMPERATURE', 0.3))

# Ansible Configuration
ANSIBLE_PLAYBOOK_PATH = "/home/phuc/zabbix-monitoring/ansible/playbooks/diagnostics/gather_system_metrics.yml"
ANSIBLE_INVENTORY_PATH = "/home/phuc/zabbix-monitoring/ansible/inventory/hosts"

# Initialize Flask
app = Flask(__name__)

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize Groq
try:
    groq_client = Groq(api_key=GROQ_API_KEY)
except Exception as e:
    logger.error(f"‚ùå Failed to initialize Groq client: {e}")
    groq_client = None

# Initialize Redis
try:
    redis_client = redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        decode_responses=True,
        socket_timeout=5
    )
    redis_client.ping()
    logger.info("‚úÖ Connected to Redis")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è  Redis connection failed: {e}, caching disabled")
    redis_client = None


class CacheManager:
    """Manage Redis caching for AI responses"""
    
    @staticmethod
    def get_cache_key(alert_data):
        """Generate cache key from alert data"""
        key_data = f"{alert_data.get('trigger', '')}{alert_data.get('severity', '')}{alert_data.get('host', '')}"
        return f"groq:{hashlib.md5(key_data.encode()).hexdigest()}"
    
    @staticmethod
    def get(key):
        """Get cached response"""
        if not redis_client:
            return None
        try:
            cached = redis_client.get(key)
            if cached:
                logger.info(f"‚úÖ Cache HIT: {key[:16]}...")
                return json.loads(cached)
        except Exception as e:
            logger.error(f"Cache get error: {e}")
        return None
    
    @staticmethod
    def set(key, value, ttl=CACHE_TTL):
        """Cache response"""
        if not redis_client:
            return
        try:
            redis_client.setex(key, ttl, json.dumps(value))
            logger.info(f"‚úÖ Cached: {key[:16]}... (TTL: {ttl}s)")
        except Exception as e:
            logger.error(f"Cache set error: {e}")

class AnsibleExecutor:
    """Execute Ansible playbooks via REST API on host machine"""
    
    # API Configuration
    ANSIBLE_API_URL = os.getenv('ANSIBLE_API_URL', 'http://host.docker.internal:5001')
    API_TIMEOUT = int(os.getenv('ANSIBLE_API_TIMEOUT', 90))

    @staticmethod
    def run_diagnostics(hostname):
        """Run diagnostics playbook via REST API"""
        try:
            api_endpoint = f"{AnsibleExecutor.ANSIBLE_API_URL}/api/v1/playbook/run"
            
            payload = {
                "playbook": "gather_system_metrics",
                "target_host": hostname,
                "extra_vars": {}
            }
            
            logger.info(f"üöÄ Calling Ansible API for {hostname}...")
            logger.info(f"   Endpoint: {api_endpoint}")
            
            response = requests.post(
                api_endpoint,
                json=payload,
                timeout=AnsibleExecutor.API_TIMEOUT
            )
            
            if response.status_code != 200:
                logger.error(f"‚ùå API returned status {response.status_code}: {response.text}")
                return None
            
            response_data = response.json()
            
            # Check execution status
            if response_data.get('status') != 'success':
                error_msg = response_data.get('error', 'Unknown error')
                logger.error(f"‚ùå Ansible execution failed: {error_msg}")
                return None
            
            # Extract result data
            result_data = response_data.get('result', {})
            logger.info(f"‚úÖ Received diagnostics data from API")
            
            return result_data
            
        except requests.exceptions.Timeout:
            logger.error(f"‚è±Ô∏è  API timeout after {AnsibleExecutor.API_TIMEOUT}s")
            return None
            
        except requests.exceptions.ConnectionError as e:
            logger.error(f"‚ùå Cannot connect to Ansible API: {e}")
            logger.error(f"   Make sure API service is running on {AnsibleExecutor.ANSIBLE_API_URL}")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Ansible API error: {e}")
            return None


class GroqAnalyzer:
    """Analyze Zabbix alerts using Groq API"""
    
    SYSTEM_PROMPT = """B·∫°n l√† m·ªôt System Administrator chuy√™n gia ƒëang ph√¢n t√≠ch alert t·ª´ h·ªá th·ªëng Zabbix monitoring.
- ƒê·ªçc d·ªØ li·ªáu th·ª±c t·∫ø t·ª´ Ansible (top, ps, df, free, netstat)
- X√°c ƒë·ªãnh nguy√™n nh√¢n g·ªëc (root cause)
- ƒê∆∞a ra khuy·∫øn ngh·ªã h√†nh ƒë·ªông c·ª• th·ªÉ
- Tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, actionable
- M·ª•c ti√™u: Gi√∫p admin nhanh ch√≥ng x·ª≠ l√Ω s·ª± c·ªë

### INPUT DATA FORMAT (t·ª´ Ansible)
B·∫°n s·∫Ω nh·∫≠n:
{
  "alert_type": "CPU|MEMORY|DISK|NETWORK",
  "hostname": "web-server-01",
  "current_value": 85,
  "threshold": 80,
  "timestamp": "2024-01-15 14:30:00",
  "ansible_output": {
    "top": "...",          // top -b -n 1 output
    "ps": "...",           // ps aux output
    "df": "...",           // df -h output
    "free": "...",         // free -h output
    "netstat": "...",      // netstat -an output (n·∫øu c√≥)
  },
  "service_info": {
    "environment": "production|staging|testing",
    "app_type": "web|api|database|cache",
    "expected_load": "normal|high|critical"
  }
}

### ANALYSIS FRAMEWORK

#### 1. ALERT TYPE: CPU
**Ph√¢n t√≠ch:**
- Ki·ªÉm tra top 3 process chi·∫øm CPU cao nh·∫•t
- So s√°nh v·ªõi baseline b√¨nh th∆∞·ªùng
- Ki·ªÉm tra context: spike t·∫°m th·ªùi hay trend tƒÉng?

**Output format:**
```
üî¥ [CRITICAL/HIGH/MEDIUM] CPU ALERT: {hostname}

üìä T√¨nh tr·∫°ng: {current_value}% / {threshold}% ng∆∞·ª°ng

‚ö° Nguy√™n nh√¢n ch√≠nh:
- [Process name] ƒëang chi·∫øm {X}% CPU
- [M√¥ t·∫£ h√†nh ƒë·ªông c·ªßa process]
- [L√Ω do t·∫°i sao n√≥ cao]

‚úÖ Khuy·∫øn ngh·ªã:
1. [H√†nh ƒë·ªông ngay l·∫≠p t·ª©c - v√≠ d·ª•: restart service, kill process]
2. [H√†nh ƒë·ªông d√†i h·∫°n - v√≠ d·ª•: scale up, optimize query]
3. [Monitoring c·∫ßn ch√∫ √Ω]

‚è±Ô∏è Urgency: [Restart now / Monitor 5min / Can wait]
```

#### 2. ALERT TYPE: MEMORY
**Ph√¢n t√≠ch:**
- Ki·ªÉm tra Used vs Available
- Top 3 process s·ª≠ d·ª•ng RAM cao nh·∫•t
- Ki·ªÉm tra swap usage - n·∫øu cao = v·∫•n ƒë·ªÅ
- Ki·ªÉm tra memory leak pattern

**Output format:**
```
üî¥ [CRITICAL/HIGH/MEDIUM] MEMORY ALERT: {hostname}

üìä T√¨nh tr·∫°ng: {current_value}% / {threshold}%

üíæ Chi ti·∫øt:
- Used: {X} GB / Total: {Y} GB
- Swap: {swap_used}% (‚ö†Ô∏è n·∫øu > 50%)
- Available: {Z} GB

‚ö° Nguy√™n nh√¢n ch√≠nh:
- [Process/Service] s·ª≠ d·ª•ng {X} GB
- [M√¥ t·∫£ v·∫•n ƒë·ªÅ]

‚úÖ Khuy·∫øn ngh·ªã:
1. [Immediate action]
2. [Follow-up action]
3. [Prevention measure]

‚è±Ô∏è Urgency: [Restart now / Monitor / Schedule maintenance]
```

#### 3. ALERT TYPE: DISK
**Ph√¢n t√≠ch:**
- Ki·ªÉm tra partition n√†o full
- Top 3 th∆∞ m·ª•c chi·∫øm space l·ªõn nh·∫•t
- Ki·ªÉm tra logs, cache, temp directories
- Inode usage (n·∫øu c√≥) - n·∫øu 100% = kh√¥ng ghi file ƒë∆∞·ª£c

**Output format:**
```
üî¥ [CRITICAL/HIGH/MEDIUM] DISK ALERT: {hostname}

üìä T√¨nh tr·∫°ng: {current_value}% / {threshold}%

üíø Chi ti·∫øt:
- Partition: {partition_name}
- Used: {X} GB / Total: {Y} GB
- Inode: {inode_percent}% ‚ö†Ô∏è

‚ö° Nguy√™n nh√¢n ch√≠nh:
- Th∆∞ m·ª•c {path} chi·∫øm {X} GB
- [M√¥ t·∫£: logs qu√° c≈©, cache kh√¥ng clear, data kh√¥ng rotate]

‚úÖ Khuy·∫øn ngh·ªã:
1. X√≥a {path}/{file_pattern} (ho·∫∑c rotate logs)
2. Ki·ªÉm tra {specific_service} configuration
3. Thi·∫øt l·∫≠p log rotation/cleanup policy

‚è±Ô∏è Urgency: [Delete now / Schedule cleanup / Monitor]
```

#### 4. ALERT TYPE: NETWORK
**Ph√¢n t√≠ch:**
- Ki·ªÉm tra connection count
- Ph√°t hi·ªán connection state b·∫•t th∆∞·ªùng (ESTABLISHED, TIME_WAIT, SYN_RECV)
- Port n√†o c√≥ traffic cao
- Ki·ªÉm tra dropped packets (n·∫øu c√≥)

**Output format:**
```
üî¥ [CRITICAL/HIGH/MEDIUM] NETWORK ALERT: {hostname}

üìä T√¨nh tr·∫°ng: {current_value}

üåê Chi ti·∫øt:
- T·ªïng connection: {total}
- ESTABLISHED: {established}
- TIME_WAIT: {time_wait}
- SYN_RECV: {syn_recv}

‚ö° Nguy√™n nh√¢n ch√≠nh:
- Port {port} c√≥ {X} connection
- [M√¥ t·∫£: client kh√¥ng close connection, slow query, DDoS signal]

‚úÖ Khuy·∫øn ngh·ªã:
1. Ki·ªÉm tra service l·∫Øng nghe port {port}
2. TƒÉng connection limit n·∫øu c·∫ßn
3. Th√™m firewall rules n·∫øu nh·∫≠n DDoS

‚è±Ô∏è Urgency: [Check immediately / Increase limits / Monitor]
```

### SPECIAL CASES & RULES

**Rule 1: Spike vs Trend**
- Spike t·∫°m th·ªùi (1-2 ph√∫t): "Monitor, c√≥ th·ªÉ l√† traffic b√¨nh th∆∞·ªùng"
- Trend tƒÉng (> 10 ph√∫t): "C·∫ßn action ngay"

**Rule 2: Correlation (n·∫øu c√≥ nhi·ªÅu alert c√πng l√∫c)**
- CPU cao + Memory cao + Disk I/O cao = Process quay v√≤ng l·∫∑p / query k√©m
- CPU cao + Network cao = C√≥ th·ªÉ DDoS ho·∫∑c malware
- Memory cao + Disk I/O cao = Swap thrashing - r·∫•t nguy hi·ªÉm

**Rule 3: Service-aware**
- nginx/httpd CPU cao: Check slow queries, client connections
- MySQL/PostgreSQL high memory: N·∫øu < 10min = query ƒë·ªôt ng·ªôt, > 30min = memory leak
- Redis memory: Clear expired keys, check LRU policy
- Docker/Kubernetes: Ki·ªÉm tra container restart loop

**Rule 4: Environment-aware**
- Production: Severity cao h∆°n, recommend restart v√†o maintenance window
- Staging: C√≥ th·ªÉ restart ngay
- Testing: C√≥ th·ªÉ t·∫°m th·ªùi ignore

**Rule 5: False Positive Detection**
- N·∫øu spike nh·ªè (< 5% v∆∞·ª£t threshold): "C√≥ th·ªÉ false positive, monitor th√™m 5 ph√∫t"
- N·∫øu baseline data kh√¥ng r√µ: "C·∫ßn baseline hi·ªÉu r√µ ƒë·ªÉ x√°c ƒë·ªãnh ch√≠nh x√°c"

### OUTPUT CONSTRAINTS
- **Length**: 150-200 words (ph√π h·ª£p Telegram message)
- **Language**: Ti·∫øng Vi·ªát, chuy√™n nghi·ªáp nh∆∞ng d·ªÖ hi·ªÉu
- **Tone**: C·∫•p b√°o nh∆∞ng kh√¥ng alarming
- **Format**: Markdown (‚úÖ, ‚ö†Ô∏è, üî¥, ‚è±Ô∏è icons)
- **Actionable**: User ph·∫£i bi·∫øt l√†m g√¨ trong 30 gi√¢y

### TONE GUIDELINES
- Tin x·∫•u ‚ùå: Kh√¥ng d√πng "server ƒëang ch·∫øt", d√πng "c·∫ßn action trong 5 ph√∫t"
- C·∫•p ƒë·ªô: "Ngay l·∫≠p t·ª©c" > "Trong 5 ph√∫t" > "Trong 1 gi·ªù" > "Schedule maintenance"
- √çch l·ª£i: Lu√¥n n√™u l·ª£i √≠ch c·ªßa action: "Restart s·∫Ω clear cache, process s·∫Ω ch·∫°y l·∫°i = system b√¨nh th∆∞·ªùng"

### EXAMPLES

**Example 1 - CPU Alert**
Input: CPU 92%, nginx process 45%, apache 20%
Output:
```
üî¥ [HIGH] CPU ALERT: web-server-01

üìä T√¨nh tr·∫°ng: 92% / 80%

‚ö° Nguy√™n nh√¢n: nginx ƒëang x·ª≠ l√Ω spike traffic (45% CPU)
- C√≥ ~500 connection t·ª´ client
- Likely: API endpoint ch·∫≠m, client ƒë·ª£i response

‚úÖ Khuy·∫øn ngh·ªã:
1. TƒÉng worker processes c·ªßa nginx t·ª´ 4 ‚Üí 8 (t·∫°m th·ªùi)
2. Check slow query log n·∫øu backend l√† PHP/Python
3. Monitor 10 ph√∫t ti·∫øp theo - n·∫øu traffic h·∫° = OK, kh√¥ng c·∫ßn restart

‚è±Ô∏è Urgency: Monitor 10 ph√∫t / T·ªëi ∆∞u configuration
```

**Example 2 - Disk Alert**
Input: Disk 95%, /var/log chi·∫øm 500GB
Output:
```
üî¥ [CRITICAL] DISK ALERT: app-server-01

üìä T√¨nh tr·∫°ng: 95% / 80%

üíø Chi ti·∫øt: /var/log = 500 GB (nguy√™n nh√¢n ch√≠nh!)
- Logs c≈© h∆°n 30 ng√†y kh√¥ng b·ªã rotate
- C√≥ multiple large log files t·ª´ nginx, syslog, app logs

‚úÖ Khuy·∫øn ngh·ªã:
1. **Ngay l·∫≠p t·ª©c**: Ch·∫°y log rotation
   `find /var/log -name "*.log.*" -mtime +30 | xargs rm`
2. Ki·ªÉm tra logrotate config - ensure weekly rotation
3. Thi·∫øt l·∫≠p max log size = 100MB ƒë·ªÉ auto rotate

‚è±Ô∏è Urgency: Delete now (an to√†n, logs c≈© c√≥ th·ªÉ x√≥a)
```
"""

    @staticmethod
    def determine_alert_type(trigger_name):
        """Determine alert type from trigger name"""
        trigger_upper = trigger_name.upper()
        if 'CPU' in trigger_upper or 'LOAD' in trigger_upper:
            return 'CPU'
        elif 'MEMORY' in trigger_upper or 'SWAP' in trigger_upper or 'RAM' in trigger_upper:
            return 'MEMORY'
        elif 'DISK' in trigger_upper or 'SPACE' in trigger_upper or 'VOLUME' in trigger_upper:
            return 'DISK'
        elif 'NETWORK' in trigger_upper or 'INTERFACE' in trigger_upper or 'BANDWIDTH' in trigger_upper:
            return 'NETWORK'
        return 'UNKNOWN'
    
    @staticmethod
    def extract_service_info(hostname, alert_data):
        """Extract service context from hostname and alert data"""
        # Default values
        service_info = {
            "environment": "production",
            "app_type": "web",
            "expected_load": "normal"
        }
        
        # Try to determine environment from hostname
        hostname_lower = hostname.lower()
        if 'prod' in hostname_lower or 'prd' in hostname_lower:
            service_info['environment'] = 'production'
        elif 'staging' in hostname_lower or 'stg' in hostname_lower:
            service_info['environment'] = 'staging'
        elif 'test' in hostname_lower or 'dev' in hostname_lower:
            service_info['environment'] = 'testing'
        
        # Try to determine app type from hostname
        if 'web' in hostname_lower or 'nginx' in hostname_lower or 'apache' in hostname_lower:
            service_info['app_type'] = 'web'
        elif 'db' in hostname_lower or 'mysql' in hostname_lower or 'postgres' in hostname_lower:
            service_info['app_type'] = 'database'
        elif 'api' in hostname_lower:
            service_info['app_type'] = 'api'
        elif 'cache' in hostname_lower or 'redis' in hostname_lower:
            service_info['app_type'] = 'cache'
        
        # Determine expected load based on severity
        severity = str(alert_data.get('severity', '')).lower()
        if 'critical' in severity or 'disaster' in severity:
            service_info['expected_load'] = 'critical'
        elif 'high' in severity or 'warning' in severity:
            service_info['expected_load'] = 'high'
        else:
            service_info['expected_load'] = 'normal'
        
        return service_info

    @staticmethod
    def analyze(alert_data, ansible_data=None):
        """Analyze alert with Groq"""
        if not groq_client:
             return {"error": "Groq client not initialized"}

        try:
            alert_type = GroqAnalyzer.determine_alert_type(alert_data.get('trigger', ''))
            hostname = alert_data.get('host', 'Unknown')
            
            # Extract service context
            service_info = GroqAnalyzer.extract_service_info(hostname, alert_data)
            
            # Prepare Ansible output - handle both dict and string
            if isinstance(ansible_data, dict):
                ansible_output = ansible_data
            elif ansible_data:
                ansible_output = {"raw": ansible_data}
            else:
                ansible_output = "No Ansible data available (Execution failed or not configured)"
            
            # Construct user message
            user_content = {
                "alert_type": alert_type,
                "hostname": hostname,
                "current_value": alert_data.get('value', 'N/A'),
                "threshold": alert_data.get('threshold', '80'),  # Default threshold
                "timestamp": alert_data.get('time', datetime.utcnow().isoformat()),
                "ansible_output": ansible_output,
                "service_info": service_info
            }
            
            logger.info(f"ü§ñ Calling Groq API for {alert_type} alert on {hostname} (env: {service_info['environment']})...")
            start_time = time.time()
            
            completion = groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {
                        "role": "system",
                        "content": GroqAnalyzer.SYSTEM_PROMPT
                    },
                    {
                        "role": "user",
                        "content": json.dumps(user_content)
                    }
                ],
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
                top_p=0.9,
                frequency_penalty=0.5
            )
            
            analysis_text = completion.choices[0].message.content
            elapsed = time.time() - start_time
            logger.info(f"‚úÖ Groq responded in {elapsed:.2f}s")
            
            return {
                "analysis": analysis_text,
                "model": "llama-3.3-70b-versatile",
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Groq API error: {e}")
            return {
                "error": str(e),
                "analysis": "AI Analysis Failed due to API Error."
            }


def require_api_key(f):
    """Decorator to check API key"""
    @wraps(f)
    def decorated(*args, **kwargs):
        if not GROQ_API_KEY:
            return jsonify({
                "error": "GROQ_API_KEY not configured",
                "status": "error"
            }), 500
        return f(*args, **kwargs)
    return decorated


@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "service": "zabbix-ai-webhook-groq",
        "timestamp": datetime.utcnow().isoformat(),
        "groq_configured": bool(GROQ_API_KEY),
        "redis_connected": redis_client is not None
    }), 200


@app.route('/webhook', methods=['POST'])
@require_api_key
def webhook():
    """Zabbix webhook endpoint"""
    try:
        data = request.get_json()
        
        # Standardize Zabbix Data
        alert_data = {
            'trigger': data.get('trigger_name', data.get('TRIGGER.NAME', 'Unknown')),
            'host': data.get('host_name', data.get('HOST.NAME', 'Unknown')),
            'severity': data.get('trigger_severity', data.get('TRIGGER.SEVERITY', 'Unknown')),
            'value': data.get('trigger_value', data.get('ITEM.VALUE', 'N/A')),
            'time': data.get('event_time', data.get('EVENT.TIME', 'N/A')),
            'description': data.get('trigger_description', data.get('TRIGGER.DESCRIPTION', '')),
            'event_id': data.get('event_id', data.get('EVENT.ID', ''))
        }
        
        logger.info(f"üì® Received alert: {alert_data['trigger']} for {alert_data['host']}")
        
        # Check cache first
        cache_key = CacheManager.get_cache_key(alert_data)
        cached_result = CacheManager.get(cache_key)
        if cached_result:
            return cached_result['analysis'], 200

        # Execute Ansible diagnostics
        ansible_data = AnsibleExecutor.run_diagnostics(alert_data['host'])
        
        # Analyze with Groq
        result = GroqAnalyzer.analyze(alert_data, ansible_data)
        
        # Cache Result
        if 'error' not in result:
             CacheManager.set(cache_key, result)
        
        # Format message with alert name
        alert_name = alert_data.get('trigger', 'Alert')
        hostname = alert_data.get('host', 'Unknown')
        message_with_header = f"**{alert_name}** on {hostname}\n\n{result['analysis']}"
        
        # Send to Telegram
        send_telegram_alert(message_with_header)
        
        return result['analysis'], 200
        
        return result['analysis'], 200
        
    except Exception as e:
        logger.error(f"‚ùå Error in /webhook: {e}")
        return f"‚ùå AI Analysis Error: {str(e)}", 500


def send_telegram_alert(message):
    """Send alert message to Telegram"""
    token = os.getenv('TELEGRAM_BOT_TOKEN')
    chat_id = os.getenv('TELEGRAM_CHAT_ID')
    
    if not token or not chat_id:
        logger.warning("‚ö†Ô∏è Telegram credentials not configured, skipping notification")
        return

    try:
        url = f"https://api.telegram.org/bot{token}/sendMessage"
        payload = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "Markdown"
        }
        response = requests.post(url, json=payload, timeout=10)
        if response.status_code == 200:
            logger.info("‚úÖ Sent Telegram notification")
        else:
            logger.error(f"‚ùå Failed to send Telegram: {response.text}")
    except Exception as e:
        logger.error(f"‚ùå Telegram send error: {e}")


if __name__ == '__main__':
    logger.info("üöÄ Starting Zabbix AI Webhook Handler (Groq Edition)")
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=os.getenv('DEBUG', 'false').lower() == 'true'
    )
