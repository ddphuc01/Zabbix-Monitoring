#!/usr/bin/env python3
"""
Zabbix AI Webhook Handler - Groq & Ansible Integration
Receives alerts from Zabbix, gathers system metrics via Ansible, and analyzes them using Groq API
"""

import os
import sys
import json
import hashlib
import time
import subprocess
import logging
from datetime import datetime
from flask import Flask, request, jsonify
from groq import Groq
import redis
import requests
from functools import wraps

# Configuration
GROQ_API_KEY = os.getenv('GROQ_API_KEY', '')
REDIS_HOST = os.getenv('REDIS_HOST', 'redis')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
CACHE_TTL = int(os.getenv('CACHE_TTL', 7200))  # Increased to 2 hours to reduce duplicate AI calls
MAX_TOKENS = int(os.getenv('MAX_TOKENS', 200))
TEMPERATURE = float(os.getenv('TEMPERATURE', 0.3))

# Alert Filtering Configuration - Skip non-critical repetitive alerts
IGNORED_SERVICES = [
    'AppXSvc',  # AppX Deployment - auto-stops when not needed
    'GoogleUpdater',  # Google auto-updater services  
    'GoogleUpdaterInternal',
    'GoogleUpdaterService',
    'edgeupdate',  # Edge updater
    'gupdate',  # Chrome updater
    'RemoteRegistry',  # Usually disabled for security
]

IGNORED_DISK_PATHS = [
    '/etc/hostname',
    '/etc/hosts',
    '/etc/resolv.conf',
    '/etc/localtime', 
    '/etc/timezone',
    '/run/secrets',  # Docker secrets mount
]

# Ansible Configuration
ANSIBLE_PLAYBOOK_PATH = "/home/phuc/zabbix-monitoring/ansible/playbooks/diagnostics/gather_system_metrics.yml"
ANSIBLE_INVENTORY_PATH = "/home/phuc/zabbix-monitoring/ansible/inventory/hosts"

# Initialize Flask
app = Flask(__name__)

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize Groq
try:
    groq_client = Groq(api_key=GROQ_API_KEY)
except Exception as e:
    logger.error(f"‚ùå Failed to initialize Groq client: {e}")
    groq_client = None

# Initialize Redis
try:
    redis_client = redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        decode_responses=True,
        socket_timeout=5
    )
    redis_client.ping()
    logger.info("‚úÖ Connected to Redis")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è  Redis connection failed: {e}, caching disabled")
    redis_client = None


def should_skip_alert(alert_data):
    """Check if alert should be skipped to save API quota"""
    trigger = alert_data.get('trigger', '')
    
    # Skip non-critical Windows services that flap frequently
    for service in IGNORED_SERVICES:
        if service.lower() in trigger.lower():
            logger.info(f"‚è≠Ô∏è  Skipping non-critical service alert: {service}")
            return True
    
    # Skip Docker mount point disk alerts
    for path in IGNORED_DISK_PATHS:
        if path in trigger:
            logger.info(f"‚è≠Ô∏è  Skipping Docker mount disk alert: {path}")
            return True
    
    return False


class CacheManager:
    """Manage Redis caching for AI responses"""
    
    @staticmethod
    def get_cache_key(alert_data):
        """Generate cache key from alert data"""
        key_data = f"{alert_data.get('trigger', '')}{alert_data.get('severity', '')}{alert_data.get('host', '')}"
        return f"groq:{hashlib.md5(key_data.encode()).hexdigest()}"
    
    @staticmethod
    def get(key):
        """Get cached response"""
        if not redis_client:
            return None
        try:
            cached = redis_client.get(key)
            if cached:
                logger.info(f"‚úÖ Cache HIT: {key[:16]}...")
                return json.loads(cached)
        except Exception as e:
            logger.error(f"Cache get error: {e}")
        return None
    
    @staticmethod
    def set(key, value, ttl=CACHE_TTL):
        """Cache response"""
        if not redis_client:
            return
        try:
            redis_client.setex(key, ttl, json.dumps(value))
            logger.info(f"‚úÖ Cached: {key[:16]}... (TTL: {ttl}s)")
        except Exception as e:
            logger.error(f"Cache set error: {e}")

class AnsibleExecutor:
    """Execute Ansible playbooks via REST API on host machine"""
    
    # API Configuration - Points to Ansible REST API service running on host
    # host.docker.internal resolves to host machine IP from within container
    ANSIBLE_API_URL = os.getenv('ANSIBLE_API_URL', 'http://host.docker.internal:5001')
    API_TIMEOUT = int(os.getenv('ANSIBLE_API_TIMEOUT', 90))

    @staticmethod
    def run_diagnostics(hostname):
        """Run diagnostics playbook via REST API"""
        try:
            api_endpoint = f"{AnsibleExecutor.ANSIBLE_API_URL}/api/v1/playbook/run"
            
            payload = {
                "playbook": "gather_system_metrics",
                "target_host": hostname,
                "extra_vars": {}
            }
            
            logger.info(f"üöÄ Calling Ansible API for {hostname}...")
            logger.info(f"   Endpoint: {api_endpoint}")
            
            response = requests.post(
                api_endpoint,
                json=payload,
                timeout=AnsibleExecutor.API_TIMEOUT
            )
            
            if response.status_code != 200:
                logger.error(f"‚ùå API returned status {response.status_code}: {response.text}")
                return None
            
            response_data = response.json()
            
            # Check execution status
            if response_data.get('status') != 'success':
                error_msg = response_data.get('error', 'Unknown error')
                logger.error(f"‚ùå Ansible execution failed: {error_msg}")
                return None
            
            # Extract result data
            result_data = response_data.get('result', {})
            logger.info(f"‚úÖ Received diagnostics data from API")
            
            return result_data
            
        except requests.exceptions.Timeout:
            logger.error(f"‚è±Ô∏è  API timeout after {AnsibleExecutor.API_TIMEOUT}s")
            return None
            
        except requests.exceptions.ConnectionError as e:
            logger.error(f"‚ùå Cannot connect to Ansible API: {e}")
            logger.error(f"   Make sure API service is running on {AnsibleExecutor.ANSIBLE_API_URL}")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Ansible API error: {e}")
            return None


class GroqAnalyzer:
    """Analyze Zabbix alerts using Groq API"""
    
    SYSTEM_PROMPT = """Ban la System Administrator phan tich Zabbix alerts.

INPUT: JSON voi alert_type, hostname, current_value, threshold, ansible_output, service_info

NHIEM VU:
1. Doc ansible_output (top, ps, df, free) de tim nguyen nhan
2. Phan tich theo alert_type: CPU/MEMORY/DISK/NETWORK/SERVICE
3. Dua khuyen nghi hanh dong cu the

OUTPUT FORMAT (150-200 words, Tieng Viet, dung emoji):
- Severity icon + Alert type + hostname
- Tinh trang hien tai vs threshold
- Nguyen nhan chinh
- Khuyen nghi cu the (commands neu can)
- Urgency level

ALERT TYPES:

CPU: Tim top 3 process tu ps aux, so voi baseline, phan biet spike vs trend

MEMORY: Check Used/Available, top RAM processes, swap usage (>50% = nguy hiem), detect memory leak

DISK: Partition nao full, top directories chiem space, check logs/cache/temp, inode usage

NETWORK: Connection count, states (ESTABLISHED/TIME_WAIT/SYN_RECV), port nao traffic cao

SERVICE: Service name, tai sao stop (crashed/disabled/manual), anh huong gi, cach start
  - Critical (DB, Web): Start ngay
  - System services: Co the doi
  - Optional (RGB, bloatware): Ignore

RULES:
- Spike <5min: "Monitor them"
- Trend >10min: "Action ngay"
- Production: Recommend maintenance window
- Staging/Testing: Restart ngay OK


### INPUT DATA FORMAT (t·ª´ Ansible)
B·∫°n s·∫Ω nh·∫≠n:
{
  "alert_type": "CPU|MEMORY|DISK|NETWORK",
  "hostname": "web-server-01",
  "current_value": 85,
  "threshold": 80,
  "timestamp": "2024-01-15 14:30:00",
  "ansible_output": {
    "top": "...",          // top -b -n 1 output
    "ps": "...",           // ps aux output
    "df": "...",           // df -h output
    "free": "...",         // free -h output
    "netstat": "...",      // netstat -an output (n·∫øu c√≥)
  },
  "service_info": {
    "environment": "production|staging|testing",
    "app_type": "web|api|database|cache",
    "expected_load": "normal|high|critical"
  }
}

### ANALYSIS FRAMEWORK

#### 1. ALERT TYPE: CPU
**Ph√¢n t√≠ch:**
- Ki·ªÉm tra top 3 process chi·∫øm CPU cao nh·∫•t
- So s√°nh v·ªõi baseline b√¨nh th∆∞·ªùng
- Ki·ªÉm tra context: spike t·∫°m th·ªùi hay trend tƒÉng?

**Output format:**
```
üî¥ [CRITICAL/HIGH/MEDIUM] CPU ALERT: {hostname}

üìä T√¨nh tr·∫°ng: {current_value}% / {threshold}% ng∆∞·ª°ng

‚ö° Nguy√™n nh√¢n ch√≠nh:
- [Process name] ƒëang chi·∫øm {X}% CPU
- [M√¥ t·∫£ h√†nh ƒë·ªông c·ªßa process]
- [L√Ω do t·∫°i sao n√≥ cao]

‚úÖ Khuy·∫øn ngh·ªã:
1. [H√†nh ƒë·ªông ngay l·∫≠p t·ª©c - v√≠ d·ª•: restart service, kill process]
2. [H√†nh ƒë·ªông d√†i h·∫°n - v√≠ d·ª•: scale up, optimize query]
3. [Monitoring c·∫ßn ch√∫ √Ω]

‚è±Ô∏è Urgency: [Restart now / Monitor 5min / Can wait]
```

#### 2. ALERT TYPE: MEMORY
**Ph√¢n t√≠ch:**
- Ki·ªÉm tra Used vs Available
- Top 3 process s·ª≠ d·ª•ng RAM cao nh·∫•t
- Ki·ªÉm tra swap usage - n·∫øu cao = v·∫•n ƒë·ªÅ
- Ki·ªÉm tra memory leak pattern

**Output format:**
```
üî¥ [CRITICAL/HIGH/MEDIUM] MEMORY ALERT: {hostname}

üìä T√¨nh tr·∫°ng: {current_value}% / {threshold}%

üíæ Chi ti·∫øt:
- Used: {X} GB / Total: {Y} GB
- Swap: {swap_used}% (‚ö†Ô∏è n·∫øu > 50%)
- Available: {Z} GB

‚ö° Nguy√™n nh√¢n ch√≠nh:
- [Process/Service] s·ª≠ d·ª•ng {X} GB
- [M√¥ t·∫£ v·∫•n ƒë·ªÅ]

‚úÖ Khuy·∫øn ngh·ªã:
1. [Immediate action]
2. [Follow-up action]
3. [Prevention measure]

‚è±Ô∏è Urgency: [Restart now / Monitor / Schedule maintenance]
```

#### 3. ALERT TYPE: DISK
**Ph√¢n t√≠ch:**
- Ki·ªÉm tra partition n√†o full
- Top 3 th∆∞ m·ª•c chi·∫øm space l·ªõn nh·∫•t
- Ki·ªÉm tra logs, cache, temp directories
- Inode usage (n·∫øu c√≥) - n·∫øu 100% = kh√¥ng ghi file ƒë∆∞·ª£c

**Output format:**
```
üî¥ [CRITICAL/HIGH/MEDIUM] DISK ALERT: {hostname}

üìä T√¨nh tr·∫°ng: {current_value}% / {threshold}%

üíø Chi ti·∫øt:
- Partition: {partition_name}
- Used: {X} GB / Total: {Y} GB
- Inode: {inode_percent}% ‚ö†Ô∏è

‚ö° Nguy√™n nh√¢n ch√≠nh:
- Th∆∞ m·ª•c {path} chi·∫øm {X} GB
- [M√¥ t·∫£: logs qu√° c≈©, cache kh√¥ng clear, data kh√¥ng rotate]

‚úÖ Khuy·∫øn ngh·ªã:
1. X√≥a {path}/{file_pattern} (ho·∫∑c rotate logs)
2. Ki·ªÉm tra {specific_service} configuration
3. Thi·∫øt l·∫≠p log rotation/cleanup policy

‚è±Ô∏è Urgency: [Delete now / Schedule cleanup / Monitor]
```

#### 4. ALERT TYPE: NETWORK
**Ph√¢n t√≠ch:**
- Ki·ªÉm tra connection count
- Ph√°t hi·ªán connection state b·∫•t th∆∞·ªùng (ESTABLISHED, TIME_WAIT, SYN_RECV)
- Port n√†o c√≥ traffic cao
- Ki·ªÉm tra dropped packets (n·∫øu c√≥)

**Output format:**
```
üî¥ [CRITICAL/HIGH/MEDIUM] NETWORK ALERT: {hostname}

üìä T√¨nh tr·∫°ng: {current_value}

üåê Chi ti·∫øt:
- T·ªïng connection: {total}
- ESTABLISHED: {established}
- TIME_WAIT: {time_wait}
- SYN_RECV: {syn_recv}

‚ö° Nguy√™n nh√¢n ch√≠nh:
- Port {port} c√≥ {X} connection
- [M√¥ t·∫£: client kh√¥ng close connection, slow query, DDoS signal]

‚úÖ Khuy·∫øn ngh·ªã:
1. Ki·ªÉm tra service l·∫Øng nghe port {port}
2. TƒÉng connection limit n·∫øu c·∫ßn
3. Th√™m firewall rules n·∫øu nh·∫≠n DDoS

‚è±Ô∏è Urgency: [Check immediately / Increase limits / Monitor]
```

#### 5. ALERT TYPE: SERVICE (Windows/Linux Service Monitoring)
**Ph√¢n t√≠ch:**
- Service name v√† tr·∫°ng th√°i (Running/Stopped)
- Ki·ªÉm tra l√Ω do service stop (auto-start disabled, crashed, manual stop)
- ·∫¢nh h∆∞·ªüng ƒë·∫øn h·ªá th·ªëng
- Service ph·ª• thu·ªôc (dependent services)

**Output format:**
```
üî¥ [CRITICAL/HIGH/MEDIUM] SERVICE ALERT: {hostname}

üìä T√¨nh tr·∫°ng: D·ªãch v·ª• "{service_name}" ƒëang stopped

‚ö° Nguy√™n nh√¢n:
- Service b·ªã stop (manual ho·∫∑c crashed)
- [N·∫øu critical service] C√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn: {dependent_features}

‚úÖ Khuy·∫øn ngh·ªã:
1. **Start l·∫°i service:** `Start-Service "{service_name}"` (Windows) ho·∫∑c `systemctl start {service_name}` (Linux)
2. Ki·ªÉm tra startup type: N√™n ƒë·∫∑t 'Automatic' n·∫øu service n√†y quan tr·ªçng
3. [N·∫øu service li√™n t·ª•c stop] Ki·ªÉm tra Event Logs/journalctl ƒë·ªÉ t√¨m l·ªói

‚è±Ô∏è Urgency: [Start now / Monitor / Can ignore if non-critical]
```

**Service Classification:**
- **Critical Services:** Database, Web Server, Application Server ‚Üí Start ngay
- **System Services:** Windows Update, Diagnostic services ‚Üí C√≥ th·ªÉ ƒë·ª£i
- **Optional Services:** RGB lighting, manufacturer bloatware ‚Üí C√≥ th·ªÉ ignore

### SPECIAL CASES & RULES

**Rule 1: Spike vs Trend**
- Spike t·∫°m th·ªùi (1-2 ph√∫t): "Monitor, c√≥ th·ªÉ l√† traffic b√¨nh th∆∞·ªùng"
- Trend tƒÉng (> 10 ph√∫t): "C·∫ßn action ngay"

**Rule 2: Correlation (n·∫øu c√≥ nhi·ªÅu alert c√πng l√∫c)**
- CPU cao + Memory cao + Disk I/O cao = Process quay v√≤ng l·∫∑p / query k√©m
- CPU cao + Network cao = C√≥ th·ªÉ DDoS ho·∫∑c malware
- Memory cao + Disk I/O cao = Swap thrashing - r·∫•t nguy hi·ªÉm

**Rule 3: Service-aware**
- nginx/httpd CPU cao: Check slow queries, client connections
- MySQL/PostgreSQL high memory: N·∫øu < 10min = query ƒë·ªôt ng·ªôt, > 30min = memory leak
- Redis memory: Clear expired keys, check LRU policy
- Docker/Kubernetes: Ki·ªÉm tra container restart loop

**Rule 4: Environment-aware**
- Production: Severity cao h∆°n, recommend restart v√†o maintenance window
- Staging: C√≥ th·ªÉ restart ngay
- Testing: C√≥ th·ªÉ t·∫°m th·ªùi ignore

**Rule 5: False Positive Detection**
- N·∫øu spike nh·ªè (< 5% v∆∞·ª£t threshold): "C√≥ th·ªÉ false positive, monitor th√™m 5 ph√∫t"
- N·∫øu baseline data kh√¥ng r√µ: "C·∫ßn baseline hi·ªÉu r√µ ƒë·ªÉ x√°c ƒë·ªãnh ch√≠nh x√°c"

### OUTPUT CONSTRAINTS
- **Length**: 150-200 words (ph√π h·ª£p Telegram message)
- **Language**: Ti·∫øng Vi·ªát, chuy√™n nghi·ªáp nh∆∞ng d·ªÖ hi·ªÉu
- **Tone**: C·∫•p b√°o nh∆∞ng kh√¥ng alarming
- **Format**: Markdown (‚úÖ, ‚ö†Ô∏è, üî¥, ‚è±Ô∏è icons)
- **Actionable**: User ph·∫£i bi·∫øt l√†m g√¨ trong 30 gi√¢y

### TONE GUIDELINES
- Tin x·∫•u ‚ùå: Kh√¥ng d√πng "server ƒëang ch·∫øt", d√πng "c·∫ßn action trong 5 ph√∫t"
- C·∫•p ƒë·ªô: "Ngay l·∫≠p t·ª©c" > "Trong 5 ph√∫t" > "Trong 1 gi·ªù" > "Schedule maintenance"
- √çch l·ª£i: Lu√¥n n√™u l·ª£i √≠ch c·ªßa action: "Restart s·∫Ω clear cache, process s·∫Ω ch·∫°y l·∫°i = system b√¨nh th∆∞·ªùng"

### EXAMPLES

**Example 1 - CPU Alert**
Input: CPU 92%, nginx process 45%, apache 20%
Output:
```
üî¥ [HIGH] CPU ALERT: web-server-01

üìä T√¨nh tr·∫°ng: 92% / 80%

‚ö° Nguy√™n nh√¢n: nginx ƒëang x·ª≠ l√Ω spike traffic (45% CPU)
- C√≥ ~500 connection t·ª´ client
- Likely: API endpoint ch·∫≠m, client ƒë·ª£i response

‚úÖ Khuy·∫øn ngh·ªã:
1. TƒÉng worker processes c·ªßa nginx t·ª´ 4 ‚Üí 8 (t·∫°m th·ªùi)
2. Check slow query log n·∫øu backend l√† PHP/Python
3. Monitor 10 ph√∫t ti·∫øp theo - n·∫øu traffic h·∫° = OK, kh√¥ng c·∫ßn restart

‚è±Ô∏è Urgency: Monitor 10 ph√∫t / T·ªëi ∆∞u configuration
```

**Example 2 - Disk Alert**
Input: Disk 95%, /var/log chi·∫øm 500GB
Output:
```
üî¥ [CRITICAL] DISK ALERT: app-server-01

üìä T√¨nh tr·∫°ng: 95% / 80%

üíø Chi ti·∫øt: /var/log = 500 GB (nguy√™n nh√¢n ch√≠nh!)
- Logs c≈© h∆°n 30 ng√†y kh√¥ng b·ªã rotate
- C√≥ multiple large log files t·ª´ nginx, syslog, app logs

‚úÖ Khuy·∫øn ngh·ªã:
1. **Ngay l·∫≠p t·ª©c**: Ch·∫°y log rotation
   `find /var/log -name "*.log.*" -mtime +30 | xargs rm`
2. Ki·ªÉm tra logrotate config - ensure weekly rotation
3. Thi·∫øt l·∫≠p max log size = 100MB ƒë·ªÉ auto rotate

‚è±Ô∏è Urgency: Delete now (an to√†n, logs c≈© c√≥ th·ªÉ x√≥a)
```
"""

    @staticmethod
    def determine_alert_type(trigger_name):
        """Determine alert type from trigger name"""
        trigger_upper = trigger_name.upper()
        if 'CPU' in trigger_upper or 'LOAD' in trigger_upper:
            return 'CPU'
        elif 'MEMORY' in trigger_upper or 'SWAP' in trigger_upper or 'RAM' in trigger_upper:
            return 'MEMORY'
        elif 'DISK' in trigger_upper or 'SPACE' in trigger_upper or 'VOLUME' in trigger_upper:
            return 'DISK'
        elif 'NETWORK' in trigger_upper or 'INTERFACE' in trigger_upper or 'BANDWIDTH' in trigger_upper:
            return 'NETWORK'
        elif 'SERVICE' in trigger_upper or 'NOT RUNNING' in trigger_upper or 'IS NOT RUNNING' in trigger_upper or 'STOPPED' in trigger_upper:
            return 'SERVICE'
        return 'UNKNOWN'
    
    @staticmethod
    def extract_service_info(hostname, alert_data):
        """Extract service context from hostname and alert data"""
        # Default values
        service_info = {
            "environment": "production",
            "app_type": "web",
            "expected_load": "normal"
        }
        
        # Try to determine environment from hostname
        hostname_lower = hostname.lower()
        if 'prod' in hostname_lower or 'prd' in hostname_lower:
            service_info['environment'] = 'production'
        elif 'staging' in hostname_lower or 'stg' in hostname_lower:
            service_info['environment'] = 'staging'
        elif 'test' in hostname_lower or 'dev' in hostname_lower:
            service_info['environment'] = 'testing'
        
        # Try to determine app type from hostname
        if 'web' in hostname_lower or 'nginx' in hostname_lower or 'apache' in hostname_lower:
            service_info['app_type'] = 'web'
        elif 'db' in hostname_lower or 'mysql' in hostname_lower or 'postgres' in hostname_lower:
            service_info['app_type'] = 'database'
        elif 'api' in hostname_lower:
            service_info['app_type'] = 'api'
        elif 'cache' in hostname_lower or 'redis' in hostname_lower:
            service_info['app_type'] = 'cache'
        
        # Determine expected load based on severity
        severity = str(alert_data.get('severity', '')).lower()
        if 'critical' in severity or 'disaster' in severity:
            service_info['expected_load'] = 'critical'
        elif 'high' in severity or 'warning' in severity:
            service_info['expected_load'] = 'high'
        else:
            service_info['expected_load'] = 'normal'
        
        return service_info

    @staticmethod
    def analyze(alert_data, ansible_data=None):
        """Analyze alert with Groq"""
        if not groq_client:
             return {"error": "Groq client not initialized"}

        try:
            alert_type = GroqAnalyzer.determine_alert_type(alert_data.get('trigger', ''))
            hostname = alert_data.get('host', 'Unknown')
            
            # Extract service context
            service_info = GroqAnalyzer.extract_service_info(hostname, alert_data)
            
            # Prepare Ansible output - handle both dict and string
            if isinstance(ansible_data, dict):
                ansible_output = ansible_data
            elif ansible_data:
                ansible_output = {"raw": ansible_data}
            else:
                ansible_output = "No Ansible data available (Execution failed or not configured)"
            
            # Construct user message
            user_content = {
                "alert_type": alert_type,
                "hostname": hostname,
                "current_value": alert_data.get('value', 'N/A'),
                "threshold": alert_data.get('threshold', '80'),  # Default threshold
                "timestamp": alert_data.get('time', datetime.utcnow().isoformat()),
                "ansible_output": ansible_output,
                "service_info": service_info
            }
            
            logger.info(f"ü§ñ Calling Groq API for {alert_type} alert on {hostname} (env: {service_info['environment']})...")
            start_time = time.time()
            
            completion = groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {
                        "role": "system",
                        "content": GroqAnalyzer.SYSTEM_PROMPT
                    },
                    {
                        "role": "user",
                        "content": json.dumps(user_content)
                    }
                ],
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
                top_p=0.9,
                frequency_penalty=0.5
            )
            
            analysis_text = completion.choices[0].message.content
            elapsed = time.time() - start_time
            logger.info(f"‚úÖ Groq responded in {elapsed:.2f}s")
            
            return {
                "analysis": analysis_text,
                "model": "llama-3.3-70b-versatile",
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Groq API error: {e}")
            return {
                "error": str(e),
                "analysis": "AI Analysis Failed due to API Error."
            }


def require_api_key(f):
    """Decorator to check API key"""
    @wraps(f)
    def decorated(*args, **kwargs):
        if not GROQ_API_KEY:
            return jsonify({
                "error": "GROQ_API_KEY not configured",
                "status": "error"
            }), 500
        return f(*args, **kwargs)
    return decorated


@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "service": "zabbix-ai-webhook-groq",
        "timestamp": datetime.utcnow().isoformat(),
        "groq_configured": bool(GROQ_API_KEY),
        "redis_connected": redis_client is not None
    }), 200


@app.route('/webhook', methods=['POST'])
@require_api_key
def webhook():
    """Zabbix webhook endpoint"""
    try:
        data = request.get_json()
        
        # Standardize Zabbix Data
        alert_data = {
            'trigger': data.get('trigger_name', data.get('TRIGGER.NAME', 'Unknown')),
            'host': data.get('host_name', data.get('HOST.NAME', 'Unknown')),
            'severity': data.get('trigger_severity', data.get('TRIGGER.SEVERITY', 'Unknown')),
            'value': data.get('trigger_value', data.get('ITEM.VALUE', 'N/A')),
            'time': data.get('event_time', data.get('EVENT.TIME', 'N/A')),
            'description': data.get('trigger_description', data.get('TRIGGER.DESCRIPTION', '')),
            'event_id': data.get('event_id', data.get('EVENT.ID', ''))
        }
        
        logger.info(f"üì® Received alert: {alert_data['trigger']} for {alert_data['host']}")
        
        # Skip non-critical repetitive alerts to save quota
        if should_skip_alert(alert_data):
            logger.info(f"‚è≠Ô∏è  Alert skipped (filtered): {alert_data['trigger']}")
            # Still send to Telegram but without diagnostics
            simple_message = f"‚ö™ **{alert_data['trigger']}**\n"
            simple_message += f"üñ•Ô∏è Host: `{alert_data['host']}`\n"
            simple_message += f"‚è∞ Time: {alert_data['time']}\n"
            simple_message += f"üìä Severity: {alert_data['severity']}\n\n"
            simple_message += "_‚ÑπÔ∏è Alert filtered (non-critical service)._"
            send_telegram_alert(simple_message, alert_data=alert_data, enable_ai_button=False)
            return "Alert filtered", 200

        # Execute Ansible diagnostics to get system metrics
        ansible_data = AnsibleExecutor.run_diagnostics(alert_data['host'])
        
        # Format message with metadata and diagnostics
        alert_name = alert_data.get('trigger', 'Alert')
        hostname = alert_data.get('host', 'Unknown')
        severity = alert_data.get('severity', 'Unknown')
        event_time = alert_data.get('time', 'N/A')
        event_id = alert_data.get('event_id', '')
        
        # Severity emoji mapping
        severity_emojis = {
            'Disaster': 'üî¥',
            'High': 'üü†',
            'Average': 'üü°',
            'Warning': 'üü¢',
            'Information': 'üîµ'
        }
        severity_emoji = severity_emojis.get(severity, '‚ö™')
        
        # Build header with metadata
        # Format datetime properly: dd/mm/yyyy HH:MM:SS
        from datetime import datetime
        try:
            # Parse and reformat time if it's in HH:MM:SS format
            if ':' in event_time and len(event_time.split(':')) == 3:
                now = datetime.now()
                formatted_time = now.strftime('%d/%m/%Y') + ' ' + event_time
            else:
                formatted_time = event_time
        except:
            formatted_time = event_time
        
        header = f"{severity_emoji} **V·∫•n ƒë·ªÅ: {alert_name}**\n"
        header += f"üñ•Ô∏è M√°y ch·ªß: `{hostname}`\n"
        header += f"‚è∞ Th·ªùi gian: {formatted_time}\n"
        header += f"üìä M·ª©c ƒë·ªô: {severity}"
        if event_id:
            header += f" | ID: `{event_id}`"
        header += "\n\n"
        
        # Add Ansible diagnostics if available
        if ansible_data and isinstance(ansible_data, dict):
            # Determine alert type from trigger name
            alert_name_lower = alert_name.lower()
            is_cpu_alert = 'cpu' in alert_name_lower or 'load' in alert_name_lower
            is_memory_alert = 'memory' in alert_name_lower or 'ram' in alert_name_lower or 'swap' in alert_name_lower
            is_disk_alert = 'disk' in alert_name_lower or 'space' in alert_name_lower or 'filesystem' in alert_name_lower
            
            header += "**üìà Th√¥ng S·ªë H·ªá Th·ªëng:**\n"
            
            metrics_found = False
            
            # NEW FORMAT: Check for structured metrics dict
            if 'metrics' in ansible_data:
                metrics = ansible_data['metrics']
                
                # ==================== CPU ALERT ====================
                if is_cpu_alert:
                    # Show CPU usage line (parse and simplify)
                    cpu_data = metrics.get('cpu', '')
                    if cpu_data:
                        for line in cpu_data.split('\n'):
                            if '%Cpu(s):' in line:
                                # Parse: %Cpu(s): 95.5 us,  4.5 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
                                # Extract key values
                                try:
                                    parts = line.split(',')
                                    us = float(parts[0].split(':')[1].strip().replace('us', '').strip())  # user
                                    sy = float(parts[1].strip().replace('sy', '').strip())  # system
                                    id_val = float(parts[3].strip().replace('id', '').strip())  # idle
                                    
                                    total_used = 100.0 - id_val
                                    
                                    # Simplified format
                                    header += f"‚Ä¢ üî• **CPU Usage:** {total_used:.1f}% s·ª≠ d·ª•ng (User: {us:.1f}%, System: {sy:.1f}% | Idle: {id_val:.1f}%)\n"
                                except:
                                    # Fallback to raw format if parsing fails
                                    header += f"‚Ä¢ üî• **CPU Usage:** {line.strip()}\n"
                                
                                metrics_found = True
                                break
                    
                    # Show TOP 10 CPU PROCESSES
                    proc_data = metrics.get('processes', '')
                    if proc_data:
                        lines = proc_data.strip().split('\n')
                        header += f"‚Ä¢ ‚ö° **Top 10 CPU Processes:**\n"
                        
                        count = 0
                        for line in lines[1:]:  # Skip header
                            if count >= 10:
                                break
                            parts = line.split()
                            if len(parts) >= 11:
                                user = parts[0]
                                cpu_pct = parts[2]
                                mem_pct = parts[3]
                                cmd = ' '.join(parts[10:])[:40]  # Truncate long commands
                                
                                # Format nicely
                                header += f"   `{count+1:2d}.` **{cpu_pct:>5s}%** CPU | {mem_pct:>4s}% RAM | `{cmd}`\n"
                                count += 1
                                metrics_found = True
                
                # ==================== MEMORY ALERT ====================
                elif is_memory_alert:
                    # Show Memory usage line
                    mem_data = metrics.get('memory', '')
                    if mem_data:
                        for line in mem_data.split('\n'):
                            if 'Mem:' in line:
                                header += f"‚Ä¢ üíæ **RAM Usage:** {line.strip()}\n"
                                metrics_found = True
                                break
                    
                    # Show TOP 10 MEMORY PROCESSES
                    proc_data = metrics.get('processes', '')
                    if proc_data:
                        # Need to re-sort by memory (column 4)
                        lines = proc_data.strip().split('\n')
                        header += f"‚Ä¢ ‚ö° **Top 10 RAM Processes:**\n"
                        
                        # Parse and sort by memory
                        process_list = []
                        for line in lines[1:]:  # Skip header
                            parts = line.split()
                            if len(parts) >= 11:
                                try:
                                    mem_pct = float(parts[3])
                                    cpu_pct = parts[2]
                                    cmd = ' '.join(parts[10:])[:40]
                                    process_list.append((mem_pct, cpu_pct, cmd))
                                except ValueError:
                                    continue
                        
                        # Sort by memory descending
                        process_list.sort(reverse=True, key=lambda x: x[0])
                        
                        for i, (mem_pct, cpu_pct, cmd) in enumerate(process_list[:10]):
                            header += f"   `{i+1:2d}.` **{mem_pct:>5.1f}%** RAM | {cpu_pct:>5s}% CPU | `{cmd}`\n"
                            metrics_found = True
                
                # ==================== DISK ALERT ====================
                elif is_disk_alert:
                    # Show ALL disk partitions sorted by usage
                    disk_data = metrics.get('disk', '')
                    if disk_data:
                        header += f"‚Ä¢ üíø **Disk Usage:**\n"
                        
                        # Parse disk lines and sort by usage%
                        disk_list = []
                        for line in disk_data.split('\n'):
                            if '/dev/' in line and '%' in line:
                                parts = line.split()
                                if len(parts) >= 6:
                                    filesystem = parts[0]
                                    size = parts[1]
                                    used = parts[2]
                                    avail = parts[3]
                                    use_pct = parts[4].rstrip('%')
                                    mount = parts[5]
                                    
                                    try:
                                        use_pct_int = int(use_pct)
                                        disk_list.append((use_pct_int, filesystem, size, used, avail, use_pct, mount))
                                    except ValueError:
                                        continue
                        
                        # Sort by usage descending
                        disk_list.sort(reverse=True, key=lambda x: x[0])
                        
                        for use_pct_int, filesystem, size, used, avail, use_pct, mount in disk_list[:5]:
                            header += f"   ‚Ä¢ `{filesystem}` **{use_pct}%** used ({used}/{size}) on `{mount}`\n"
                            metrics_found = True
                
                # ==================== GENERIC ALERT (show summary) ====================
                else:
                    # Show brief summary of all metrics
                    cpu_data = metrics.get('cpu', '')
                    if cpu_data:
                        for line in cpu_data.split('\n'):
                            if '%Cpu(s):' in line:
                                header += f"‚Ä¢ üî• CPU: {line.strip()}\n"
                                metrics_found = True
                                break
                    
                    mem_data = metrics.get('memory', '')
                    if mem_data:
                        for line in mem_data.split('\n'):
                            if 'Mem:' in line:
                                header += f"‚Ä¢ üíæ RAM: {line.strip()}\n"
                                metrics_found = True
                                break
                    
                    disk_data = metrics.get('disk', '')
                    if disk_data:
                        for line in disk_data.split('\n'):
                            if '/dev/' in line and '%' in line:
                                parts = line.split()
                                if len(parts) >= 5:
                                    header += f"‚Ä¢ üíø Disk: {parts[0]} {parts[4]} used\n"
                                    metrics_found = True
                                    break
                    
                    proc_data = metrics.get('processes', '')
                    if proc_data:
                        lines = proc_data.strip().split('\n')
                        if len(lines) > 1:
                            parts = lines[1].split()
                            if len(parts) >= 11:
                                cpu_pct = parts[2]
                                cmd = ' '.join(parts[10:])[:30]
                                header += f"‚Ä¢ ‚ö° Top Process: {cmd} ({cpu_pct}%)\n"
                                metrics_found = True
            
            # OLD FORMAT FALLBACK: Try parsing stdout/stderr
            elif 'stdout' in ansible_data or 'stderr' in ansible_data:
                stdout = ansible_data.get('stdout', '')
                stderr = ansible_data.get('stderr', '')
                
                # Try to parse as JSON or plain text (old code path)
                try:
                    if stdout and isinstance(stdout, str):
                        # Try to parse as JSON
                        ansible_json = json.loads(stdout)
                        
                        # Extract from plays -> tasks -> hosts -> msg
                        if 'plays' in ansible_json:
                            for play in ansible_json['plays']:
                                if 'tasks' in play:
                                    for task in play['tasks']:
                                        if 'hosts' in task:
                                            for host_name, host_data in task['hosts'].items():
                                                if 'msg' in host_data and isinstance(host_data['msg'], list):
                                                    # msg is a list with sections
                                                    current_section = None
                                                    for line in host_data['msg']:
                                                        if '=== CPU ===' in line:
                                                            current_section = 'cpu'
                                                        elif '=== MEMORY ===' in line:
                                                            current_section = 'memory'
                                                        elif '=== DISK ===' in line:
                                                            current_section = 'disk'
                                                        elif current_section and line.strip():
                                                            # Extract key metrics
                                                            if current_section == 'cpu' and '%Cpu' in line:
                                                                header += f"‚Ä¢ üî• CPU: {line.strip()}\n"
                                                                metrics_found = True
                                                            elif current_section == 'memory' and 'Mem:' in line:
                                                                header += f"‚Ä¢ üíæ RAM: {line.strip()}\n"
                                                                metrics_found = True
                                                            elif current_section == 'disk' and '/dev/' in line and '%' in line:
                                                                parts = line.split()
                                                                if len(parts) >= 5:
                                                                    header += f"‚Ä¢ üíø Disk: {parts[0]} {parts[4]} used\n"
                                                                    metrics_found = True
                                                                    break
                except (json.JSONDecodeError, Exception) as e:
                    logger.error(f"Error parsing old format Ansible output: {e}")
            
            # If no specific metrics found, show generic message
            if not metrics_found:
                if 'status' in ansible_data and ansible_data.get('status') == 'success':
                    header += f"‚Ä¢ ‚úÖ Ansible ƒë√£ ch·∫°y th√†nh c√¥ng\n"
                    header += f"‚Ä¢ üìä Nh·∫•n 'Ph√¢n T√≠ch AI' b√™n d∆∞·ªõi ƒë·ªÉ nh·∫≠n khuy·∫øn ngh·ªã chi ti·∫øt\n"
                else:
                    header += f"‚Ä¢ ‚úÖ Ansible ƒë√£ ch·∫°y th√†nh c√¥ng\n"
                    header += f"‚Ä¢ üìä Nh·∫•n 'Ch·∫°y Ch·∫©n ƒêo√°n' ƒë·ªÉ xem chi ti·∫øt\n"
            
            header += "\n"
        
        # Add footer note about AI
        header += "_üí° Nh·∫•n 'Ph√¢n T√≠ch AI' b√™n d∆∞·ªõi ƒë·ªÉ nh·∫≠n khuy·∫øn ngh·ªã chi ti·∫øt._"
        
        # Store alert+ansible data in cache for AI button later
        cache_key = f"alert_data:{event_id}"
        if redis_client:
            try:
                full_alert_data = {
                    'alert': alert_data,
                    'ansible': ansible_data
                }
                redis_client.setex(cache_key, 3600, json.dumps(full_alert_data))
                logger.info(f"üíæ Cached alert data: {cache_key}")
            except Exception as e:
                logger.error(f"Failed to cache alert data: {e}")
        
        # Send to Telegram with AI analysis button
        send_telegram_alert(header, alert_data=alert_data, enable_ai_button=True)
        
        return "Alert sent (AI on-demand)", 200
        
    except Exception as e:
        logger.error(f"‚ùå Error in /webhook: {e}")
        return f"‚ùå AI Analysis Error: {str(e)}", 500


def send_telegram_alert(message, alert_data=None, enable_ai_button=False):
    """Send alert message to Telegram with inline keyboard buttons"""
    token = os.getenv('TELEGRAM_BOT_TOKEN')
    chat_id = os.getenv('TELEGRAM_CHAT_ID')
    
    if not token or not chat_id:
        logger.warning("‚ö†Ô∏è Telegram credentials not configured, skipping notification")
        return

    try:
        url = f"https://api.telegram.org/bot{token}/sendMessage"
        
        # Build inline keyboard with action buttons
        keyboard = None
        if alert_data:
            hostname = alert_data.get('host', 'Unknown')
            trigger_name = alert_data.get('trigger', '')
            event_id = alert_data.get('event_id', '')
            
            # Determine alert type for appropriate buttons
            alert_type = GroqAnalyzer.determine_alert_type(trigger_name)
            
            buttons = []
            
            # AI Analysis button (only if enabled)
            if enable_ai_button and event_id:
                buttons.append(
                    [{"text": "ü§ñ Get AI Analysis", "callback_data": f"ai_analysis:{event_id}"}]
                )
            
            # Service-specific buttons
            if alert_type == 'SERVICE':
                # Extract service name from trigger (e.g., "Service XYZ is not running")
                service_name = trigger_name.split('"')[1] if '"' in trigger_name else 'Unknown'
                buttons.append(
                    [{"text": "üîÑ Restart Service", "callback_data": f"restart_service:{hostname}:{service_name}"}]
                )
                buttons.append(
                    [{"text": "üìä Check Status", "callback_data": f"check_service:{hostname}:{service_name}"}]
                )
            else:
                # Generic diagnostic button for other alert types
                buttons.append(
                    [{"text": "üîç Run Diagnostics", "callback_data": f"diagnostics:{hostname}"}]
                )
            
            # Common buttons for all alerts
            buttons.append([
                {"text": "‚úÖ Acknowledge", "callback_data": f"ack:{event_id}"},
                {"text": "üîï Ignore", "callback_data": f"ignore:{event_id}"}
            ])
            
            keyboard = {"inline_keyboard": buttons}
        
        payload = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "Markdown"
        }
        
        # Add keyboard if available
        if keyboard:
            payload["reply_markup"] = keyboard
        
        response = requests.post(url, json=payload, timeout=10)
        if response.status_code == 200:
            logger.info("‚úÖ Sent Telegram notification with inline buttons")
            
            # Cache original alert message for "Back to Alert" button
            if redis_client and alert_data and event_id:
                try:
                    response_data = response.json()
                    if response_data.get('ok'):
                        message_id = response_data['result']['message_id']
                        
                        # Store original alert with buttons for restoration
                        original_alert_data = {
                            'message_text': message,
                            'message_id': message_id,
                            'buttons': buttons if keyboard else []
                        }
                        
                        cache_key = f"original_alert:{event_id}"
                        redis_client.setex(cache_key, 3600, json.dumps(original_alert_data))
                        logger.info(f"üíæ Cached original alert: {cache_key}")
                except Exception as e:
                    logger.error(f"Failed to cache original alert: {e}")
        else:
            logger.error(f"‚ùå Failed to send Telegram: {response.text}")
    except Exception as e:
        logger.error(f"‚ùå Telegram send error: {e}")


if __name__ == '__main__':
    logger.info("üöÄ Starting Zabbix AI Webhook Handler (Groq Edition)")
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=os.getenv('DEBUG', 'false').lower() == 'true'
    )
